
+-----------------------------+
|                             |
| SUMMARY                     |
|                             |
+-----------------------------+

A crash-recovery TOF bug in HBase which results in data loss due to unexpected node crash. The triggering is similar with HB5 but regionserver crashes at one different point. In deleteNodeRecursively(), regionserver first deletes the lock,  then HLog file path znode and finally HLog file directory znode. For triggering HB5, we makes it crashes when the second file is deleted, but it crashes when the third file is deleted for HB6.


+-----------------------------+
|                             |
| HOW TO INSTALL              |
|                             |
+-----------------------------+

1. Download hbase-0.90.1 source
-------------------------------
# tar zxvf hbase-0.90.1-orig-src.tar.gz

2. Copy and Deploy two hbase clusters
-------------------------------
# mv hbase-0.90.1-orig-src hbase-1
# cp -r hbase-1 hbase-2

3. Configure hbase replication
-------------------------------
# cp hbase-config/conf-1/* hbase-1/conf/
# cp hbase-config/conf-2/* hbase-2/conf/*

4. Prepare data for observation
-------------------------------
# cd <hbase-triggering-dir>
# ./data/prepareData.sh

Note: for simplicity, we have prepared data at first and stored them in ./data/data-1 and ./data/data-2.

5. Apply the patch
-------------------------------
# cd hbase-1
# patch -p1 -i HB6.patch

6. Compile
-------------------------------
# hbase-1/compile.sh


+-----------------------------+
|                             |
| Reproduce the bug           |
|                             |
+-----------------------------+

1. Run the triggering script
-------------------------------
# ./HB6-trigger.sh

2. Check result
-------------------------------
In the output information of triggering script, it can be found that the data of one table hb6 in master cluster cannot be migrated to slave one when regionserver crashes right after deleting HLog file directory's znode. In this situation, if the master cluster encounters outage, the data will be lost in both clusters. 
